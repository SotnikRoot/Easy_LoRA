import os
import torch
import json
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model
from datasets import Dataset

# ============================================================
# –ù–ê–°–¢–†–û–ô–ö–ò
# ============================================================

MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
MODEL_DIR = "./models"
JSON_FILE = "result_LoRA.json"
OUTPUT_DIR = "./qwen-lora-finetuned_2.0"

# –†–µ–∂–∏–º—ã —Å–∫–æ—Ä–æ—Å—Ç–∏ (–≤—ã–±–µ—Ä–∏—Ç–µ –æ–¥–∏–Ω):
SPEED_MODE = "ULTRA_FAST"  # "ULTRA_FAST", "FAST", "BALANCED", "QUALITY"

SPEED_CONFIGS = {
    "ULTRA_FAST": {
        "epochs": 1,
        "batch_size": 4,
        "grad_accum": 2,
        "max_length": 256,
        "lora_r": 4,
        "learning_rate": 3e-4,
        "logging_steps": 50,
        "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å (~4-5 —á–∞—Å–æ–≤ –Ω–∞ intel i5)"
    },
    "FAST": {
        "epochs": 2,
        "batch_size": 2,
        "grad_accum": 4,
        "max_length": 512,
        "lora_r": 8,
        "learning_rate": 2e-4,
        "logging_steps": 25,
        "description": "–ë—ã—Å—Ç—Ä–æ —Å —Ö–æ—Ä–æ—à–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º (~15-17 —á–∞—Å–æ–≤ –Ω–∞ intel i5)"
    },
    "BALANCED": {
        "epochs": 3,
        "batch_size": 1,
        "grad_accum": 8,
        "max_length": 512,
        "lora_r": 12,
        "learning_rate": 1e-4,
        "logging_steps": 20,
        "description": "–ë–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ (~29-35 —á–∞—Å–æ–≤ –Ω–∞ intel i5)"
    },
    "QUALITY": {
        "epochs": 5,
        "batch_size": 1,
        "grad_accum": 8,
        "max_length": 1024,
        "lora_r": 16,
        "learning_rate": 5e-5,
        "logging_steps": 10,
        "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (~60-70 —á–∞—Å–æ–≤ –Ω–∞ intel i5)"
    }
}

MODE = SPEED_CONFIGS[SPEED_MODE]

# ============================================================
# –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
# ============================================================

def load_and_format_json_data(json_file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç JSON –¥–∞–Ω–Ω—ã–µ"""
    print(f"üìñ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑ {json_file_path}...")
    
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        exit()
    
    training_examples = []
    
    for item in data["data"]:
        instruction = item.get('instruction', '').strip()
        output = item.get('output', '').strip()
        input_text = item.get('input', '').strip()
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        if not instruction or not output:
            continue
        
        # –§–æ—Ä–º–∞—Ç –≤ —Å—Ç–∏–ª–µ ChatML (–±–æ–ª–µ–µ —á–∏—Å—Ç—ã–π)
        if input_text:
            text = f"<|im_start|>system\n–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ 1–°:–ü—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ.–≠–ª–µ–º–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –∏ –ø–æ –¥–µ–ª—É.<|im_end|>\n<|im_start|>user\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {input_text}\n\n–í–æ–ø—Ä–æ—Å: {instruction}<|im_end|>\n<|im_start|>assistant\n{output}<|im_end|>"
        else:
            text = f"<|im_start|>system\n–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ 1–°:–ü—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ.–≠–ª–µ–º–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –∏ –ø–æ –¥–µ–ª—É.<|im_end|>\n<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n{output}<|im_end|>"
        
        training_examples.append(text)
    
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(training_examples)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä
    if training_examples:
        print("\n‚úÖ –ü—Ä–∏–º–µ—Ä –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö:")
        print("-" * 60)
        print(training_examples[0][:500])
        print("-" * 60 + "\n")
    
    return training_examples

# ============================================================
# –°–û–ó–î–ê–ù–ò–ï –ü–ê–ü–û–ö
# ============================================================

print("üìÅ –°–æ–∑–¥–∞—é –ø–∞–ø–∫–∏...")
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ============================================================
# –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò
# ============================================================

print(f"üì• –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å {MODEL_NAME}...")

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    cache_dir=MODEL_DIR,
    trust_remote_code=True
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    cache_dir=MODEL_DIR,
    torch_dtype=torch.float32,
    device_map="cpu",
    trust_remote_code=True
)

print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

# ============================================================
# –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
# ============================================================

text_examples = load_and_format_json_data(JSON_FILE)

if len(text_examples) < 10:
    print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
    print(f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–ª—å–∫–æ {len(text_examples)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 100+ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")

dataset_dict = {"text": text_examples}
dataset = Dataset.from_dict(dataset_dict)

def tokenize_function(examples):
    """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    result = tokenizer(
        examples["text"],
        truncation=True,
        max_length=MODE["max_length"],  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        padding="max_length",
        return_tensors=None
    )
    result["labels"] = result["input_ids"].copy()
    return result

print("üîÑ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É—é –¥–∞–Ω–Ω—ã–µ...")
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text"],
    desc="–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
)

# –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train –∏ eval
split_dataset = tokenized_dataset.train_test_split(test_size=0.005, seed=42)

print(f"üìä Train: {len(split_dataset['train'])}, Eval: {len(split_dataset['test'])}")

# ============================================================
# –ù–ê–°–¢–†–û–ô–ö–ê LoRA (–±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è)
# ============================================================

print("üîß –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—é LoRA...")

lora_config = LoraConfig(
    r=MODE["lora_r"],
    lora_alpha=32,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 16 –¥–æ 32
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ============================================================
# –£–õ–£–ß–®–ï–ù–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø
# ============================================================

print("‚öôÔ∏è –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—é –æ–±—É—á–µ–Ω–∏–µ...")

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    
    # –ö–†–ò–¢–ò–ß–ù–û: –±–æ–ª—å—à–µ —ç–ø–æ—Ö!
    num_train_epochs=MODE["epochs"],
    
    # –ú–µ–Ω—å—à–µ batch size –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    per_device_train_batch_size=MODE["batch_size"],
    per_device_eval_batch_size=MODE["batch_size"],
    gradient_accumulation_steps=MODE["grad_accum"],
    
    # Learning rate
    learning_rate=MODE["learning_rate"],  # –ë—ã–ª–æ 2e-4, —Å–Ω–∏–∂–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    warmup_steps=MODE["logging_steps"],  # –ë—ã–ª–æ 10
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    logging_steps=MODE["logging_steps"],
    save_steps=MODE["logging_steps"],
    eval_steps=MODE["logging_steps"],
    save_total_limit=3,
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    weight_decay=0.01,
    max_grad_norm=1.0,
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ
    fp16=False,
    dataloader_num_workers=0,
    report_to="none",
    
    # –í–∞–∂–Ω–æ!
    remove_unused_columns=False,
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=split_dataset["train"],
    eval_dataset=split_dataset["test"],
    data_collator=data_collator,
)

# ============================================================
# –û–ë–£–ß–ï–ù–ò–ï
# ============================================================

print("\n" + "="*60)
print("üöÄ –ù–ê–ß–ò–ù–ê–Æ –û–ë–£–ß–ï–ù–ò–ï")
print("="*60 + "\n")

trainer.train()

print("\n‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!\n")

# ============================================================
# –°–û–•–†–ê–ù–ï–ù–ò–ï
# ============================================================

print("üíæ –°–æ—Ö—Ä–∞–Ω—è—é –º–æ–¥–µ–ª—å...")
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {OUTPUT_DIR}\n")

# ============================================================
# –£–õ–£–ß–®–ï–ù–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï
# ============================================================

print("="*60)
print("üß™ –¢–ï–°–¢–ò–†–£–Æ –ú–û–î–ï–õ–¨")
print("="*60 + "\n")

model.eval()

def generate_text(prompt, max_new_tokens=256):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è"""
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ –æ–±—É—á–µ–Ω–∏—è
    formatted_prompt = f"<|im_start|>system\n–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ 1–°:–ü—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ.–≠–ª–µ–º–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –∏ –ø–æ –¥–µ–ª—É.<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    inputs = tokenizer(formatted_prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,  # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
            temperature=0.3,  # –°–Ω–∏–∂–µ–Ω–æ —Å 0.7 –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
            top_p=0.85,  # –°–Ω–∏–∂–µ–Ω–æ —Å 0.9
            top_k=40,  # –î–æ–±–∞–≤–ª–µ–Ω–æ
            repetition_penalty=1.15,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 1.1
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            no_repeat_ngram_size=3,  # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —Ñ—Ä–∞–∑
        )
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é —á–∞—Å—Ç—å
    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ assistant
    if "<|im_start|>assistant" in full_output:
        response = full_output.split("<|im_start|>assistant")[-1]
        response = response.replace("<|im_end|>", "").strip()
    else:
        response = full_output[len(formatted_prompt):].strip()
    
    return response

# –¢–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã
test_prompts = [
    "–ß—Ç–æ —Ç–∞–∫–æ–µ 1–°:–ü—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ.–≠–ª–µ–º–µ–Ω—Ç?",
    "–ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ?",
    "–†–∞—Å—Å–∫–∞–∂–∏ –æ –ø–∞–Ω–µ–ª–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è"
]

for i, prompt in enumerate(test_prompts, 1):
    print(f"–¢–µ—Å—Ç {i}: {prompt}")
    print("-" * 60)
    try:
        response = generate_text(prompt)
        print(response)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}")
    print("-" * 60 + "\n")

print("üéâ –ì–æ—Ç–æ–≤–æ!")
